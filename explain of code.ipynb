{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "1dbf69b4-bf1a-4310-a67a-79a8c5bd480c",
   "metadata": {},
   "outputs": [],
   "source": [
    "1. Install Dependencies and Setup\n",
    "\n",
    "# This is a command in Python that uses pip, a package manager for Python, to install the TensorFlow library.\n",
    "\n",
    "!pip install tensorflow \n",
    "\n",
    "#The code imports the TensorFlow library, which is a tool for creating and training machine learning models.\n",
    "\n",
    "import tensorflow as tf\n",
    "\n",
    "#This code imports the os module in Python, which provides a way for Python programs to interact with the operating system. \n",
    "\n",
    "import os\n",
    "\n",
    "#This code uses the TensorFlow function list_physical_devices() to obtain a list of available GPUs on the machine.\n",
    "#The list can be used to configure TensorFlow to utilize the GPUs for faster computation in deep learning models.\n",
    "\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "\n",
    "#This code block checks if GPUs are available and sets dynamic memory allocation for each GPU using the TensorFlow function set_memory_growth().\n",
    "#This code optimizes TensorFlow to use available GPUs and improve the performance of deep learning models.\n",
    "\n",
    "if gpus:\n",
    "    try:\n",
    "        # Set memory growth for each GPU\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "            \n",
    "        # Verify memory growth is set\n",
    "        logical_gpus = tf.config.experimental.list_logical_devices('GPU')\n",
    "        print(len(gpus), \"Physical GPUs,\", len(logical_gpus), \"Logical GPUs\")\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)\n",
    "\n",
    "#This code obtains a list of available GPUs using TensorFlow's list_physical_devices() function \n",
    "#and configures TensorFlow to use them for faster deep learning model computation.\n",
    "\n",
    "tf.config.list_physical_devices('GPU')\n",
    "\n",
    "2. Remove Dodgy Images \n",
    "\n",
    "#This code imports the cv2 module, which allows developers to use OpenCV's image and video processing functions in Python programs.\n",
    "\n",
    "import cv2\n",
    "\n",
    "#The code imports the imghdr module in Python, which can be used to check the type of image file based on its content.\n",
    "#It provides functions to determine the image file type or test if a file is an image file.\n",
    "\n",
    "import imghdr\n",
    "\n",
    "#This code creates a variable named data_dir that stores the path to a directory containing data used by a Python program.\n",
    "#It makes the code more flexible and maintainable by storing the path in a variable.\n",
    "\n",
    "data_dir = 'data'\n",
    "\n",
    "#This code creates a list of common image file extensions used to filter files in a directory or perform other image processing tasks.\n",
    "\n",
    "image_exts = ['jpeg', 'jpg', 'bmp', 'png']\n",
    "\n",
    "#This code filters image files in a directory and its subdirectories.\n",
    "#It reads images with OpenCV and uses imghdr to determine their types. Invalid files are removed with os.remove().\n",
    "\n",
    "for image_class in os.listdir(data_dir): \n",
    "    for image in os.listdir(os.path.join(data_dir, image_class)):\n",
    "        image_path = os.path.join(data_dir, image_class, image)\n",
    "        try: \n",
    "            img = cv2.imread(image_path)\n",
    "            tip = imghdr.what(image_path)\n",
    "            if tip not in image_exts: \n",
    "                print('Image not in ext list {}'.format(image_path))\n",
    "                os.remove(image_path)\n",
    "        except Exception as e: \n",
    "            print('Issue with image {}'.format(image_path))\n",
    "            # os.remove(image_path)\n",
    "\n",
    "3.Load Data\n",
    "\n",
    "#This code imports the pyplot module from the matplotlib package and renames it to plt. pyplot provides functions for creating and customizing plots and renaming.\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "#Code imports numpy package. Numpy is used for working with numerical data arrays and provides many math functions.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "#This code uses image_dataset_from_directory() from tf.keras.utils to load and preprocess image data from a directory for use in machine learning models.\n",
    "#It automatically splits the data into training and validation sets and returns a tf.data.Dataset object containing batches of images and labels.\n",
    "\n",
    "data = tf.keras.utils.image_dataset_from_directory('data')\n",
    "\n",
    "#This code creates a numpy iterator over a tf.data.Dataset object, which enables the user to iterate over the dataset and convert it to numpy format. \n",
    "#This can be useful for operations like visualization or data manipulation, but may not be suitable for large datasets.\n",
    "\n",
    "data_iterator = data.as_numpy_iterator()\n",
    "\n",
    "##This code uses next() to get the next batch of data and labels from a tf.data.Dataset object via a numpy iterator created using the as_numpy_iterator() method.\n",
    "\n",
    "batch = data_iterator.next()\n",
    "\n",
    "#This code prepares the image data for deep learning by creating a visualization of the data and normalizing the pixel values.\n",
    "\n",
    "fig, ax = plt.subplots(ncols=4, figsize=(20,20))\n",
    "for idx, img in enumerate(batch[0][:4]):\n",
    "    ax[idx].imshow(img.astype(int))\n",
    "    ax[idx].title.set_text(batch[1][idx])\n",
    "\n",
    "4. Scale Data\n",
    "\n",
    "#This code applies a lambda function to the data using the map() method of the tf.data.Dataset object.\n",
    "#The lambda function normalizes the pixel values of the images by dividing them by 255, which scales the pixel values to the range [0,1].\n",
    "\n",
    "data = data.map(lambda x,y: (x/255, y))\n",
    "\n",
    "#This code returns the first batch of data and labels from the tf.data.Dataset object as numpy arrays using the as_numpy_iterator() method and next() function.\n",
    "\n",
    "data.as_numpy_iterator().next()\n",
    "\n",
    "5. Split Data\n",
    "\n",
    "#This code defines the sizes of the training, validation, and test sets as a percentage of the total size of the data.\n",
    "#The training set is 70% of the data, the validation set is 20%, and the test set is 10%.\n",
    "\n",
    "train_size = int(len(data)*.7)\n",
    "val_size = int(len(data)*.2)\n",
    "test_size = int(len(data)*.1)\n",
    "\n",
    "#train_size is a variable that represents the size of the training dataset, which is calculated by taking 70% of the total length of the original dataset.\n",
    "\n",
    "train_size\n",
    "\n",
    "#The code splits a dataset into three subsets: training, validation, and testing, based on the sizes specified. \n",
    "#The training set is taken from the beginning of the dataset, followed by the validation set, and finally the test set. \n",
    "\n",
    "train = data.take(train_size)\n",
    "val = data.skip(train_size).take(val_size)\n",
    "test = data.skip(train_size+val_size).take(test_size)\n",
    "\n",
    "6. Build Deep Learning Model\n",
    "\n",
    "#The 'train' dataset contains 70% of the preprocessed data and is intended for training a machine learning model.\n",
    "#The code splits the data into three subsets, allowing the model to be trained on one subset while being evaluated on the others.\n",
    "\n",
    "train\n",
    "\n",
    "#Sequential is a class in tensorflow.keras.models for building a linear stack of layers in a deep learning model.\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "\n",
    "#Conv2D, MaxPooling2D, Dense, Flatten, and Dropout are classes in tensorflow.keras.layers used for creating convolutional, max pooling, \\\n",
    "#fully connected, flattening, and dropout layers, respectively. \n",
    "\n",
    "\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout\n",
    "\n",
    "#The code initializes an instance of the Sequential class in Keras and assigns it to the variable \"model\".\n",
    "\n",
    "model = Sequential()\n",
    "\n",
    "#The code creates a CNN for image classification using a popular architecture.\n",
    "#It has three Convolutional layers with max pooling, a Flatten layer, two fully connected layers, and an output layer. ReLU and sigmoid activation functions are used.\n",
    "\n",
    "model.add(Conv2D(16, (3,3), 1, activation='relu', input_shape=(256,256,3)))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(32, (3,3), 1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Conv2D(16, (3,3), 1, activation='relu'))\n",
    "model.add(MaxPooling2D())\n",
    "model.add(Flatten())\n",
    "model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "\n",
    "#This code compiles the Keras model by specifying the optimizer, loss function, and evaluation metrics to be used during training. \n",
    "#The optimizer used is 'adam', which is a popular optimization algorithm.\n",
    "#The loss function used is Sparse Categorical Cross-entropy, which is often used for multi-class classification tasks. \n",
    "\n",
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "\n",
    "model.compile(optimizer='adam', loss=SparseCategoricalCrossentropy(), metrics=['accuracy'])\n",
    "\n",
    "\n",
    "#model.summary() provides a summary of the layers in a Keras model, including the layer type, output shape, and number of parameters.\n",
    "\n",
    "model.summary()\n",
    "\n",
    "7. Train\n",
    "\n",
    "#The code trains a Keras machine learning model with training and validation data using the fit() method, and includes a callback to TensorBoard for monitoring training progress. \n",
    "#The output is a History object containing information about the training process.\n",
    "\n",
    "logdir='logs'\n",
    "\n",
    "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)\n",
    "\n",
    "hist = model.fit(train, epochs=20, validation_data=val, callbacks=[tensorboard_callback])\n",
    "\n",
    "8. Plot Performance\n",
    "\n",
    "#The code plots the training and validation loss of a machine learning model over epochs.\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(hist.history['loss'], color='teal', label='loss')\n",
    "plt.plot(hist.history['val_loss'], color='orange', label='val_loss')\n",
    "fig.suptitle('Loss', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "#The code plots the training and validation accuracy over epochs.\n",
    "\n",
    "fig = plt.figure()\n",
    "plt.plot(hist.history['accuracy'], color='teal', label='accuracy')\n",
    "plt.plot(hist.history['val_accuracy'], color='orange', label='val_accuracy')\n",
    "fig.suptitle('Accuracy', fontsize=20)\n",
    "plt.legend(loc=\"upper left\")\n",
    "plt.show()\n",
    "\n",
    "9. Evaluate\n",
    "\n",
    "#Precision, Recall, and BinaryAccuracy are evaluation metrics for binary classification models that measure the model's ability to avoid false positives, detect positive cases, and accuracy rate\n",
    "\n",
    "from tensorflow.keras.metrics import Precision, Recall, CategoricalAccuracy\n",
    "\n",
    "#Precision, Recall, and BinaryAccuracy are metrics used to evaluate classification models. They are imported from the tensorflow.keras.metrics library\n",
    "\n",
    "pre = Precision()\n",
    "re = Recall()\n",
    "acc = CategoricalAccuracy()\n",
    "\n",
    "#This code calculates the test accuracy of a trained model using the CategoricalAccuracy metric. \n",
    "#It loops through the test dataset batches and updates the metric object with the ground truth and predicted labels.\n",
    "\n",
    "from tensorflow.keras.metrics import CategoricalAccuracy\n",
    "\n",
    "# create metric objects\n",
    "categorical_accuracy = CategoricalAccuracy()\n",
    "\n",
    "for batch in test.as_numpy_iterator(): \n",
    "    X, y = batch\n",
    "    yhat = model.predict(X)\n",
    "    categorical_accuracy.update_state(y, yhat)\n",
    "\n",
    "print(f'Test Categorical Accuracy: {categorical_accuracy.result().numpy()}')\n",
    "\n",
    "\n",
    "#This will print the precision, recall and binary accuracy metrics of the trained model on the test dataset.\n",
    "\n",
    "print(pre.result(), re.result(), acc.result())\n",
    "\n",
    "10. Test\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#This code reads an image file and displays it using matplotlib.\n",
    "#If the image is read successfully, it is converted from BGR to RGB using OpenCV's cv2.cvtColor() function, and then displayed using plt.imshow() and plt.show().\n",
    "#If the image cannot be read, the code prints a message indicating that it could not read the image.\n",
    "\n",
    "img = cv2.imread('6.jpg')\n",
    "if img is not None:\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img_array = np.asarray(img)\n",
    "    plt.imshow(img_array)\n",
    "    plt.show()\n",
    "else:\n",
    "    print('Could not read image')\n",
    "\n",
    "#The code resizes an input image to a new size of (256, 256) using the tf.image.resize() function, then displays the resulting image using plt.imshow().\n",
    "\n",
    "resize = tf.image.resize(img, (256,256))\n",
    "plt.imshow(resize.numpy().astype(int))\n",
    "plt.show()\n",
    "\n",
    "#This code uses a pre-trained model to predict the class of an input image by taking the argmax of the predicted class probabilities and retrieving the class name from a list of class names.\n",
    "\n",
    "class_names = [\"ABBOTTS BABBLER\", \"ABYSSINIAN GROUND HORNBILL\", \"BALD EAGLE\"]\n",
    "yhat = model.predict(np.expand_dims(resize / 255, 0))\n",
    "\n",
    "# get predicted class index\n",
    "class_idx = tf.argmax(yhat, axis=1)[0]\n",
    "\n",
    "predicted_class = class_names[class_idx]\n",
    "print(f\"Predicted class is {predicted_class}\")\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gputensorflow",
   "language": "python",
   "name": "gputensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
